{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "from performance_helper import get_performance_scores, get_finetuned_scores, compute_f1_scores_quad, compute_scores_single, merge_aspect_lists\n",
    "from table_helper import create_tabular\n",
    "from table_boldener import bolden_table_max_values_with_hline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import itertools\n",
    "# import shutil\n",
    "# import io, re\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SEEDS = 5\n",
    "TASKS = [\"tasd\", \"asqp\"]\n",
    "DATASETS = [\"rest15\", \"rest16\", \"flightabsa\", \"coursera\", \"hotels\"]\n",
    "METHODS = [\"paraphrase\", \"dlo\"]\n",
    "AUG_TECHNIQUES = [\"eda\", \"llm_eda\", \"back_translation\"]\n",
    "\n",
    "raw_dataset_to_formatted = {\"rest16\": \"Rest16\", \"rest15\": \"Rest15\", \"flightabsa\": \"FlightABSA\", \"gerest\": \"GERest\", \"hotels\": \"OATS Hotels\"}\n",
    "format_dataset_to_raw = {\"Rest16\": \"rest16\", \"Rest15\": \"rest15\", \"FlightABSA\": \"flightabsa\", \"GERest\": \"gerest\", \"OATS Hotels\": \"hotels\"}\n",
    "raw_method_to_formatted = {\"paraphrase\": \"Paraphrase \\citep{zhang2021aspect}\", \"dlo\": \"DLO \\citep{hu2022improving}\", \"mvp\": \"MVP \\citep{gou2023mvp}\"}\n",
    "format_method_to_raw = {\"Paraphrase \\citep{zhang2021aspect}\": \"paraphrase\", \"DLO \\citep{hu2022improving}\": \"dlo\", \"MVP \\citep{gou2023mvp}\": \"mvp\"}\n",
    "raw_aug_to_formatted = {\"eda\": \"EDA\", \"llm_eda\": \"LLM-EDA\", \"back_translation\": \"Back-Translation\", \"-\": \"-\", \"llm_annotator\": \"LLM-Annotator\"}\n",
    "format_aug_to_raw = {\"EDA\": \"eda\", \"LLM-EDA\": \"llm_eda\", \"Back-Translation\": \"back_translation\", \"-\": \"-\", \"LLM-Annotator\": \"llm_annotator\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_element_scores(loaded_json, task):\n",
    "    labels = loaded_json[\"all_labels\"]\n",
    "    preds = loaded_json[\"all_preds\"]\n",
    "    seed_scores = compute_f1_scores_quad(preds, labels)\n",
    "    seed_scores_ac = compute_scores_single(preds, labels, \"single_ac\")\n",
    "    seed_scores_at = compute_scores_single(preds, labels, \"single_at\")\n",
    "    seed_scores_pol = compute_scores_single(preds, labels, \"single_pol\")\n",
    "\n",
    "    seed_scores[\"ac\"] = seed_scores_ac\n",
    "    seed_scores[\"at\"] = seed_scores_at\n",
    "    seed_scores[\"pol\"] = seed_scores_pol\n",
    "    if task == \"asqp\":\n",
    "        seed_scores_ot = compute_scores_single(preds, labels, \"single_ot\")\n",
    "        seed_scores[\"ot\"] = seed_scores_ot\n",
    "    return seed_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores):\n",
    "    averages = {}\n",
    "    for key in scores[0].keys():\n",
    "        if isinstance(scores[0][key], dict):  # Falls geschachtelte Dicts vorhanden sind\n",
    "            averages[key] = {subkey: np.mean([s[key][subkey] for s in scores]) for subkey in scores[0][key]}\n",
    "        else:\n",
    "            averages[key] = np.mean([s[key] for s in scores])\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load LLM-annotated fine-tuned scores\n",
    "scores_llm_ann_train = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for method in METHODS:\n",
    "            for fs in [0, 10, 50]:\n",
    "                for n_ann_ex in [800, \"full\"]:\n",
    "\n",
    "                    scores = []\n",
    "                    for seed in range(N_SEEDS):\n",
    "                        with open(\n",
    "                            f\"../_out_fine_tunings/01_llm_annotate_train/{method}_{n_ann_ex}_{task}_{fs}_{dataset}_{seed}.json\"\n",
    "                        ) as f:\n",
    "                            loaded_json = json.load(f)\n",
    "                            seed_scores = add_element_scores(loaded_json, task)\n",
    "                            scores.append(seed_scores)\n",
    "                    scores_llm_ann_train[\n",
    "                        f\"{method}_{n_ann_ex}_{task}_{fs}_{dataset}\"\n",
    "                    ] = calc_mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Augmented fine-tuned scores\n",
    "scores_traditional_aug = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for aug in AUG_TECHNIQUES:\n",
    "            for method in METHODS:\n",
    "                for fs in [10, 50]:\n",
    "                    for n_ann_ex in [1600, 800, \"full\"]:\n",
    "\n",
    "                        scores = []\n",
    "                        for seed in range(N_SEEDS):\n",
    "                            with open(\n",
    "                                f\"../_out_fine_tunings/03_traditional_augmentation/{method}_{aug}_{n_ann_ex}_{task}_{fs}_{dataset}_{seed}.json\"\n",
    "                            ) as f:\n",
    "                                loaded_json = json.load(f)\n",
    "                                seed_scores = add_element_scores(loaded_json, task)\n",
    "                                scores.append(seed_scores)\n",
    "                        scores_traditional_aug[\n",
    "                            f\"{method}_{aug}_{n_ann_ex}_{task}_{fs}_{dataset}\"\n",
    "                        ] = calc_mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Fine-tuned LLM scores\n",
    "scores_fine_tune_llm = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for fs in [0, 10, 50]:\n",
    "            for n_ann_ex in [800, \"full\"]:\n",
    "                scores = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    with open(\n",
    "                        f\"../_out_fine_tunings/02_fine_tune_llm/gemma-3-4b_{seed}_{task}_{fs}_{dataset}_{n_ann_ex}.json\"\n",
    "                    ) as f:\n",
    "                        loaded_json_raw = json.load(f)\n",
    "\n",
    "                        loaded_json = {\n",
    "                            \"all_preds\": [j[\"pred_label\"] for j in loaded_json_raw],\n",
    "                            \"all_labels\": [j[\"tuple_list\"] for j in loaded_json_raw],\n",
    "                        }\n",
    "\n",
    "                        seed_scores = add_element_scores(loaded_json, task)\n",
    "\n",
    "                        scores.append(seed_scores)\n",
    "                scores_fine_tune_llm[f\"gemma-3-4b_{task}_{fs}_{dataset}_{n_ann_ex}\"] = (\n",
    "                    calc_mean(scores)\n",
    "                )\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for n_ann_ex in [800, \"full\"]:\n",
    "            scores = []\n",
    "            for seed in range(N_SEEDS):\n",
    "                with open(\n",
    "                    f\"../_out_fine_tunings/02_fine_tune_llm/gemma-3-4b_{seed}_{task}_{dataset}_{n_ann_ex}.json\"\n",
    "                ) as f:\n",
    "                    loaded_json_raw = json.load(f)\n",
    "\n",
    "                    loaded_json = {\n",
    "                        \"all_preds\": [j[\"pred_label\"] for j in loaded_json_raw],\n",
    "                        \"all_labels\": [j[\"tuple_list\"] for j in loaded_json_raw],\n",
    "                    }\n",
    "\n",
    "                    seed_scores = add_element_scores(loaded_json, task)\n",
    "\n",
    "                    scores.append(seed_scores)\n",
    "            scores_fine_tune_llm[f\"gemma-3-4b_{task}_{dataset}_{n_ann_ex}\"] = calc_mean(\n",
    "                scores\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load methods baselines\n",
    "scores_00_baseline = {}\n",
    "\n",
    "with open(\"../past_results.json\") as f:\n",
    "    past_results = json.load(f)\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for method in METHODS:\n",
    "            for n_ann_ex in [10, 50, 800, \"full\"]:\n",
    "\n",
    "                scores = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    if n_ann_ex == \"full\":\n",
    "                        file_path = f\"../_out_paper_1/00_baselines/training_{task}_{dataset}_seed-{seed}_n-train_{method}.json\"\n",
    "                    else:\n",
    "                        file_path = f\"../_out_paper_1/00_baselines/training_{task}_{dataset}_seed-{seed}_n-train_{method}_{n_ann_ex}.json\"\n",
    "                    with open(file_path) as f:\n",
    "                        loaded_json = json.load(f)\n",
    "                        seed_scores = add_element_scores(loaded_json, task)\n",
    "                        scores.append(seed_scores)\n",
    "                scores_mean = calc_mean(scores)\n",
    "\n",
    "                scores_00_baseline[f\"{method}_{n_ann_ex}_{task}_{dataset}\"] = (\n",
    "                    scores_mean\n",
    "                )\n",
    "\n",
    "                for metric in [\"f1\", \"precision\", \"recall\"]:\n",
    "                    if n_ann_ex == \"full\":\n",
    "                        try:\n",
    "                            scores_00_baseline[f\"{method}_{n_ann_ex}_{task}_{dataset}\"][\n",
    "                                metric\n",
    "                            ] = past_results[task][method][dataset][metric]\n",
    "                        except:\n",
    "                            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load zero-shot scores\n",
    "scores_zeroshot = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for fs in [0, 10, 50]:\n",
    "                scores = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    with open(\n",
    "                        f\"../_out_paper_1/zeroshot/{task}_{dataset}_test_gemma3:27b_{seed}_label_{fs}.json\"\n",
    "                    ) as f:\n",
    "                        loaded_json_raw = json.load(f)\n",
    "\n",
    "                        loaded_json = {\n",
    "                            \"all_preds\": [j[\"pred_label\"] for j in loaded_json_raw],\n",
    "                            \"all_labels\": [j[\"tuple_list\"] for j in loaded_json_raw],\n",
    "                        }\n",
    "\n",
    "                        seed_scores = add_element_scores(loaded_json, task)\n",
    "\n",
    "                        scores.append(seed_scores)\n",
    "                scores_zeroshot[f\"{task}_{fs}_{dataset}\"] = calc_mean(scores)\n",
    "\n",
    "# WITH SELF-Consistency\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for fs in [0, 10, 50]:\n",
    "                all_example_data = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    with open(\n",
    "                        f\"../_out_paper_1/zeroshot/{task}_{dataset}_test_gemma3:27b_{seed}_label_{fs}.json\"\n",
    "                    ) as f:\n",
    "                        loaded_json_raw = json.load(f)\n",
    "\n",
    "                        loaded_json = {\n",
    "                            \"all_preds\": [j[\"pred_label\"] for j in loaded_json_raw],\n",
    "                            \"all_labels\": [j[\"tuple_list\"] for j in loaded_json_raw],\n",
    "                        }\n",
    "\n",
    "                        all_example_data.append(loaded_json)\n",
    "\n",
    "                all_labels = all_example_data[0][\"all_labels\"]\n",
    "                all_preds = [[] for _ in range(len(all_labels))]\n",
    "                for seed in range(0, N_SEEDS):\n",
    "                    for idx in range(len(all_labels)):\n",
    "                        all_preds[idx].append(all_example_data[seed][\"all_preds\"][idx])\n",
    "                        if seed == N_SEEDS - 1:\n",
    "                            all_preds[idx] = merge_aspect_lists(all_preds[idx])\n",
    "                            all_preds[idx] = [list(p) for p in all_preds[idx]]\n",
    "\n",
    "                loaded_json = {\n",
    "                    \"all_preds\": all_preds,\n",
    "                    \"all_labels\": all_labels,\n",
    "                }\n",
    "\n",
    "                scores = add_element_scores(loaded_json, task)\n",
    "                scores_zeroshot[f\"{task}_{fs}_{dataset}_sc\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 59\n",
      "Augmentation Strategy 59\n",
      "\\# Train 59\n",
      "\\# Few-shot 59\n",
      "rest15_f1 0\n",
      "rest15_precision 0\n",
      "rest15_recall 0\n",
      "rest16_f1 0\n",
      "rest16_precision 0\n",
      "rest16_recall 0\n",
      "flightabsa_f1 0\n",
      "flightabsa_precision 0\n",
      "flightabsa_recall 0\n",
      "coursera_f1 0\n",
      "coursera_precision 0\n",
      "coursera_recall 0\n",
      "hotels_f1 0\n",
      "hotels_precision 0\n",
      "hotels_recall 0\n"
     ]
    }
   ],
   "source": [
    "table_data_total = {\n",
    "    \"Method\": [raw_method_to_formatted[\"paraphrase\"]] * 28\n",
    "    + [raw_method_to_formatted[\"dlo\"]] * 28\n",
    "    + [\"\\\\multicolumn{1}{l}{\\\\begin{tabular}[c]{@{}l@{}}Gemma-3-27B\\\\ Prompting\\end{tabular}}\"] * 3,\n",
    "    \"Augmentation Strategy\": (\n",
    "        [raw_aug_to_formatted[\"llm_annotator\"]] * 6\n",
    "        + [raw_aug_to_formatted[\"eda\"]] * 6\n",
    "        + [raw_aug_to_formatted[\"llm_eda\"]] * 6\n",
    "        + [raw_aug_to_formatted[\"back_translation\"]] * 6\n",
    "        + [\"-\"] * 4\n",
    "        + [raw_aug_to_formatted[\"llm_annotator\"]] * 6\n",
    "        + [raw_aug_to_formatted[\"eda\"]] * 6\n",
    "        + [raw_aug_to_formatted[\"llm_eda\"]] * 6\n",
    "        + [raw_aug_to_formatted[\"back_translation\"]] * 6\n",
    "        + [\"-\"] * 4\n",
    "        + [\"-\"] * 3\n",
    "    ),\n",
    "    \"\\# Train\": (\n",
    "        [\"800\"] * 3\n",
    "        + [\"Full\"] * 3\n",
    "        + ([\"800\"] * 2 + [\"Full\"] * 2 + [\"1,600\"] * 2) * 3\n",
    "        + [\"10\", \"50\", \"800\", \"Full\"]\n",
    "    )\n",
    "    * 2\n",
    "    + [\"-\"] * 3,\n",
    "    \"\\# Few-shot\": ([\"0\", \"10\", \"50\"] * 2 + [\"10\", \"50\"] * 9 + [\"-\"] * 4) * 2\n",
    "    + [\"0\", \"10\", \"50\"],\n",
    "}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for metric in [\"f1\", \"precision\", \"recall\"]:\n",
    "        table_data_total[f\"{dataset}_{metric}\"] = []\n",
    "\n",
    "for key in table_data_total.keys():\n",
    "    print(key, len(table_data_total[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_llm_ann_train\n",
    "# scores_traditional_aug\n",
    "# scores_fine_tune_llm\n",
    "# scores_00_baseline\n",
    "# scores_zeroshot\n",
    "TASK = \"asqp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"f1\", \"precision\", \"recall\"]:\n",
    "    for dataset in DATASETS:\n",
    "        for method in METHODS:\n",
    "            for n_ann_ex in [800, \"full\"]:\n",
    "                for fs in [0, 10, 50]:\n",
    "                    table_data_total[f\"{dataset}_{metric}\"].append(\n",
    "                        scores_llm_ann_train[\n",
    "                            f\"{method}_{n_ann_ex}_{TASK}_{fs}_{dataset}\"\n",
    "                        ][metric]\n",
    "                    )\n",
    "            for aug_method in AUG_TECHNIQUES:\n",
    "                for n_ann_ex in [800, \"full\", 1600]:\n",
    "                    for fs in [10, 50]:\n",
    "                        table_data_total[f\"{dataset}_{metric}\"].append(\n",
    "                            scores_traditional_aug[\n",
    "                                f\"{method}_{aug_method}_{n_ann_ex}_{TASK}_{fs}_{dataset}\"\n",
    "                            ][metric]\n",
    "                        )\n",
    "            for n_ann_ex in [10, 50, 800, \"full\"]:\n",
    "                table_data_total[f\"{dataset}_{metric}\"].append(\n",
    "                    scores_00_baseline[f\"{method}_{n_ann_ex}_{TASK}_{dataset}\"][metric]\n",
    "                )\n",
    "\n",
    "        for fs in [0, 10, 50]:\n",
    "            table_data_total[f\"{dataset}_{metric}\"].append(\n",
    "                scores_zeroshot[f\"{TASK}_{fs}_{dataset}_sc\"][metric]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Method', 'Augmentation Strategy', '\\\\# Train', '\\\\# Few-shot', 'rest15_f1', 'rest15_precision', 'rest15_recall', 'rest16_f1', 'rest16_precision', 'rest16_recall', 'flightabsa_f1', 'flightabsa_precision', 'flightabsa_recall', 'coursera_f1', 'coursera_precision', 'coursera_recall', 'hotels_f1', 'hotels_precision', 'hotels_recall'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_data_total.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\hline\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{\\multirow{28}{*}{Paraphrase \\citep{zhang2021aspect}}} & \\textbf{\\multirow{6}{*}{LLM-Annotator}} & \\textbf{\\multirow{3}{*}{800}} & 0 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 10 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{3}{*}{Full}} & 0 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 10 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{6}{*}{EDA}} & \\textbf{\\multirow{2}{*}{800}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{Full}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{1,600}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{6}{*}{LLM-EDA}} & \\textbf{\\multirow{2}{*}{800}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{Full}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{1,600}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{6}{*}{Back-Translation}} & \\textbf{\\multirow{2}{*}{800}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{Full}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{1,600}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{4}{*}{-}} & \\textbf{\\multirow{1}{*}{10}} & - & 1.32 & 1.64 & 1.11 & 3.56 & 4.02 & 3.23 & 3.44 & 4.34 & 2.85 & 4.75 & 5.35 & 4.26 & 2.63 & 3.66 & 2.06 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{1}{*}{50}} & - & 25.55 & 24.58 & 26.62 & 23.50 & 23.75 & 23.25 & 17.98 & 18.58 & 17.42 & 19.38 & 20.72 & 18.21 & 23.09 & 23.67 & 22.59 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{1}{*}{800}} & - & 46.32 & 45.61 & 47.07 & 56.88 & 55.65 & 58.17 & 54.96 & 54.10 & 55.86 & 30.79 & 30.63 & 30.96 & 53.65 & 52.57 & 54.77 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{1}{*}{Full}} & - & 46.93 & 46.16 & 47.72 & 57.93 & 56.63 & 59.30 & 57.76 & 57.37 & 58.17 & 32.34 & 32.06 & 32.63 & 53.87 & 52.61 & 55.19 \\\\\n",
      "\\hline\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{\\multirow{28}{*}{DLO \\citep{hu2022improving}}} & \\textbf{\\multirow{6}{*}{LLM-Annotator}} & \\textbf{\\multirow{3}{*}{800}} & 0 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 10 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{3}{*}{Full}} & 0 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 10 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 & 46.33 & 46.47 & 46.18 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{6}{*}{EDA}} & \\textbf{\\multirow{2}{*}{800}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{Full}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{1,600}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{6}{*}{LLM-EDA}} & \\textbf{\\multirow{2}{*}{800}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{Full}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{1,600}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{6}{*}{Back-Translation}} & \\textbf{\\multirow{2}{*}{800}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{Full}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{2}{*}{1,600}} & 10 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 & 31.38 & 31.60 & 31.16 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{2-19}\\arrayrulecolor{black}\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{\\multirow{7}{*}{-}} & \\textbf{\\multirow{1}{*}{10}} & - & 3.88 & 4.14 & 3.65 & 4.49 & 4.79 & 4.23 & 4.46 & 5.63 & 3.69 & 5.88 & 6.62 & 5.30 & 3.60 & 3.89 & 3.36 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{1}{*}{50}} & - & 27.93 & 25.94 & 30.26 & 29.63 & 29.23 & 30.06 & 28.81 & 28.14 & 29.53 & 19.87 & 21.64 & 18.37 & 28.14 & 29.46 & 26.93 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{1}{*}{800}} & - & 48.76 & 47.54 & 50.04 & 58.59 & 56.65 & 60.68 & 56.91 & 55.33 & 58.58 & 29.96 & 29.54 & 30.40 & 53.97 & 53.23 & 54.73 \\\\\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{} & \\textbf{} & \\textbf{\\multirow{1}{*}{Full}} & - & 48.18 & 47.08 & 49.33 & 59.79 & 57.92 & 61.80 & 58.61 & 56.85 & 60.47 & 32.01 & 31.48 & 32.55 & 55.62 & 54.45 & 56.84 \\\\\n",
      "\\hline\n",
      "\\arrayrulecolor{gray}\\cline{3-19}\\arrayrulecolor{black}\n",
      "\\textbf{\\multirow{3}{*}{\\multicolumn{1}{l}{\\begin{tabular}[c]{@{}l@{}}Gemma-3-27B\\ Prompting\\end{tabular}}}} & \\textbf{} & \\textbf{\\multirow{3}{*}{-}} & 0 & 29.74 & 30.74 & 28.81 & 38.60 & 40.14 & 37.17 & 38.96 & 41.21 & 36.95 & 13.40 & 13.89 & 12.95 & 29.27 & 32.49 & 26.63 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 10 & 39.34 & 41.69 & 37.23 & 41.11 & 43.72 & 38.80 & 44.44 & 50.00 & 40.00 & 22.81 & 28.70 & 18.92 & 30.92 & 38.59 & 25.80 \\\\\n",
      "\\textbf{} & \\textbf{} & \\textbf{} & 50 & 40.62 & 46.66 & 35.97 & 51.24 & 56.97 & 46.56 & 49.91 & 56.14 & 44.92 & 27.20 & 36.99 & 21.51 & 40.24 & 52.57 & 32.59 \\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = [\n",
    "    {\"name\": \"Method\", \"type\": \"string\", \"struct\": True},\n",
    "    {\"name\": \"Augmentation Strategy\", \"type\": \"string\", \"struct\": True},\n",
    "    {\"name\": \"\\# Train\", \"type\": \"string\", \"struct\": True},\n",
    "    {\"name\": \"\\# Few-shot\", \"type\": \"string\", \"struct\": False},\n",
    "]\n",
    "for ds_name in DATASETS:\n",
    "    for metric in [\"f1\", \"precision\", \"recall\"]:\n",
    "        rows += [\n",
    "            {\n",
    "                \"name\": f\"{ds_name}_{metric}\",\n",
    "                \"type\": \"double\",\n",
    "                \"struct\": False,\n",
    "                \"decimal_place\": 2,\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "output = io.StringIO()\n",
    "sys.stdout = output\n",
    "create_tabular(rows=rows, data=table_data_total)\n",
    "printed_output = output.getvalue()\n",
    "sys.stdout = sys.__stdout__\n",
    "output.close()\n",
    "print(printed_output)\n",
    "# print(bolden_table_max_values_with_hline(printed_output, [0, 1, 2, 3]))\n",
    "\n",
    "with open(\"summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(bolden_table_max_values_with_hline(printed_output, [0, 1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nils_hellwig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
