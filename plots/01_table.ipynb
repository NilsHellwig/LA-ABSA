{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99b6dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Add paths for custom modules\n",
    "sys.path.append(os.path.abspath(\"../../zero-shot-absa-quad\"))\n",
    "sys.path.append(os.path.abspath(\"../../zero-shot-absa-quad/plots\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dd97a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "from performance_helper import compute_f1_scores_quad, compute_scores_single, merge_aspect_lists\n",
    "from table_tool import insert_line, display_table, round_numbers, minimize, bolden, bolden_column, bolden_column_header\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import itertools\n",
    "# import shutil\n",
    "# import io, re\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bb6d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SEEDS = 5\n",
    "TASKS = [\"tasd\", \"asqp\"]\n",
    "DATASETS = [\"rest15\", \"rest16\", \"flightabsa\", \"coursera\", \"hotels\"]\n",
    "METHODS = [\"dlo\", \"llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit\"]\n",
    "AUG_TECHNIQUES = [\"eda\", \"qaie\"]\n",
    "\n",
    "raw_dataset_to_formatted = {\"rest16\": \"Rest16\", \"rest15\": \"Rest15\", \"flightabsa\": \"FlightABSA\", \"coursera\": \"OATS Coursera\", \"hotels\": \"OATS Hotels\"}\n",
    "format_dataset_to_raw = {\"Rest16\": \"rest16\", \"Rest15\": \"rest15\", \"FlightABSA\": \"flightabsa\", \"coursera\": \"OATS Coursera\", \"OATS Hotels\": \"hotels\"}\n",
    "raw_method_to_formatted = {\"llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit\": \"FT-LLama-3-8B\", \"dlo\": \"DLO \\citep{hu2022improving}\"}\n",
    "format_method_to_raw = {\"FT-LLama-3-8B\": \"llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit\", \"DLO \\citep{hu2022improving}\": \"dlo\"}\n",
    "raw_aug_to_formatted = {\"eda\": \"EDA\", \"QAIE\": \"QAIE\", \"llm_annotator\": \"LLM-Annotator\"}\n",
    "format_aug_to_raw = {\"EDA\": \"eda\", \"-\": \"-\", \"LLM-Annotator\": \"llm_annotator\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e504f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_element_scores(loaded_json, task):\n",
    "    labels = loaded_json[\"all_labels\"]\n",
    "    preds = loaded_json[\"all_preds\"]\n",
    "    seed_scores = compute_f1_scores_quad(preds, labels)\n",
    "    seed_scores_ac = compute_scores_single(preds, labels, \"single_ac\")\n",
    "    seed_scores_at = compute_scores_single(preds, labels, \"single_at\")\n",
    "    seed_scores_pol = compute_scores_single(preds, labels, \"single_pol\")\n",
    "\n",
    "    seed_scores[\"ac\"] = seed_scores_ac\n",
    "    seed_scores[\"at\"] = seed_scores_at\n",
    "    seed_scores[\"pol\"] = seed_scores_pol\n",
    "    if task == \"asqp\":\n",
    "        seed_scores_ot = compute_scores_single(preds, labels, \"single_ot\")\n",
    "        seed_scores[\"ot\"] = seed_scores_ot\n",
    "    return seed_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e108dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean(scores):\n",
    "    averages = {}\n",
    "    for key in scores[0].keys():\n",
    "        if isinstance(scores[0][key], dict):  # Falls geschachtelte Dicts vorhanden sind\n",
    "            averages[key] = {subkey: np.mean([s[key][subkey] for s in scores]) for subkey in scores[0][key]}\n",
    "        else:\n",
    "            averages[key] = np.mean([s[key] for s in scores])\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d542179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load LLM-annotated fine-tuned scores\n",
    "scores_llm_ann_train = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for method in METHODS:\n",
    "            for fs in [0, 10, 50]:\n",
    "                for n_ann_ex in [\"full\"]:\n",
    "\n",
    "                    scores = []\n",
    "                    for seed in range(N_SEEDS):\n",
    "                        with open(\n",
    "                            f\"../_out_fine_tunings/01_llm_annotate_train/{method}_{n_ann_ex}_{task}_{fs}_{dataset}_{seed}.json\"\n",
    "                        ) as f:\n",
    "                            loaded_json = json.load(f)\n",
    "                            seed_scores = add_element_scores(loaded_json, task)\n",
    "                            scores.append(seed_scores)\n",
    "                    scores_llm_ann_train[\n",
    "                        f\"{method}_{n_ann_ex}_{task}_{fs}_{dataset}\"\n",
    "                    ] = calc_mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d08fc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dlo_full_tasd_0_rest15', 'dlo_full_tasd_10_rest15', 'dlo_full_tasd_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_rest15', 'dlo_full_asqp_0_rest15', 'dlo_full_asqp_10_rest15', 'dlo_full_asqp_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_rest15', 'dlo_full_tasd_0_rest16', 'dlo_full_tasd_10_rest16', 'dlo_full_tasd_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_rest16', 'dlo_full_asqp_0_rest16', 'dlo_full_asqp_10_rest16', 'dlo_full_asqp_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_rest16', 'dlo_full_tasd_0_flightabsa', 'dlo_full_tasd_10_flightabsa', 'dlo_full_tasd_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_flightabsa', 'dlo_full_asqp_0_flightabsa', 'dlo_full_asqp_10_flightabsa', 'dlo_full_asqp_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_flightabsa', 'dlo_full_tasd_0_coursera', 'dlo_full_tasd_10_coursera', 'dlo_full_tasd_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_coursera', 'dlo_full_asqp_0_coursera', 'dlo_full_asqp_10_coursera', 'dlo_full_asqp_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_coursera', 'dlo_full_tasd_0_hotels', 'dlo_full_tasd_10_hotels', 'dlo_full_tasd_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_hotels', 'dlo_full_asqp_0_hotels', 'dlo_full_asqp_10_hotels', 'dlo_full_asqp_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_hotels'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_llm_ann_train.keys(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3b699c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Augmented fine-tuned scores\n",
    "scores_traditional_aug = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for fs in [10, 50]:\n",
    "                scores = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    path = f\"../../QAIE-ABSA-2025-adaption/03_results/{task}_{dataset}_fs_{fs}_{seed}.json\"\n",
    "\n",
    "                    with open(path) as f:\n",
    "                        loaded_json = json.load(f)\n",
    "                        seed_scores = add_element_scores(loaded_json, task)\n",
    "                        scores.append(seed_scores)\n",
    "                scores_traditional_aug[\n",
    "                    f\"qaie_{task}_{fs}_{dataset}\"\n",
    "                ] = calc_mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2130bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for aug in [\"eda\"]:\n",
    "            for method in METHODS:\n",
    "                for fs in [10, 50]:\n",
    "                    for n_ann_ex in [2, 5, 10]:\n",
    "                        scores = []\n",
    "                        for seed in range(N_SEEDS):\n",
    "                            path = f\"../_out_fine_tunings/03_traditional_augmentation/{method}_{aug}_{n_ann_ex}_{task}_{fs}_{dataset}_{seed}.json\"\n",
    "                            with open(\n",
    "                                path\n",
    "                            ) as f:\n",
    "                                loaded_json = json.load(f)\n",
    "                                seed_scores = add_element_scores(loaded_json, task)\n",
    "                                scores.append(seed_scores)\n",
    "                        scores_traditional_aug[\n",
    "                            f\"{method}_{aug}_{n_ann_ex}_{task}_{fs}_{dataset}\"\n",
    "                        ] = calc_mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "70bdcef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load methods baselines\n",
    "scores_00_baseline = {}\n",
    "\n",
    "with open(\"../../zero-shot-absa-quad/plots/past_results.json\") as f:\n",
    "    past_results = json.load(f)\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for method in METHODS:\n",
    "            for n_ann_ex in [10, 50, \"full\"]:\n",
    "\n",
    "                scores = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    if n_ann_ex == \"full\":\n",
    "                        file_path = f\"../../zero-shot-absa-quad/generations/00_baselines/training_{task}_{dataset}_seed-{seed}_n-train_{method}.json\"\n",
    "                    else:\n",
    "                        file_path = f\"../../zero-shot-absa-quad/generations/00_baselines/training_{task}_{dataset}_seed-{seed}_n-train_{method}_{n_ann_ex}.json\"\n",
    "                    with open(file_path) as f:\n",
    "                        loaded_json = json.load(f)\n",
    "                        seed_scores = add_element_scores(loaded_json, task)\n",
    "                        scores.append(seed_scores)\n",
    "                scores_mean = calc_mean(scores)\n",
    "\n",
    "                scores_00_baseline[f\"{method}_{n_ann_ex}_{task}_{dataset}\"] = (\n",
    "                    scores_mean\n",
    "                )\n",
    "\n",
    "                for metric in [\"f1\", \"precision\", \"recall\"]:\n",
    "                    if n_ann_ex == \"full\":\n",
    "                        try:\n",
    "                            scores_00_baseline[f\"{method}_{n_ann_ex}_{task}_{dataset}\"][\n",
    "                                metric\n",
    "                            ] = past_results[task][method][dataset][metric]\n",
    "                        except:\n",
    "                            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66c12908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Nachtr√§glicher Filter zero/few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acaf7c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load zero-shot scores\n",
    "scores_zeroshot = {}\n",
    "\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for fs in [0, 10, 20, 30, 40, 50]:\n",
    "                scores = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    with open(\n",
    "                        f\"../../zero-shot-absa-quad/generations/zeroshot/{task}_{dataset}_test_gemma3:27b_{seed}_label_{fs}.json\"\n",
    "                    ) as f:\n",
    "                        loaded_json_raw = json.load(f)\n",
    "\n",
    "                        loaded_json = {\n",
    "                            \"all_preds\": [j[\"pred_label\"] for j in loaded_json_raw],\n",
    "                            \"all_labels\": [j[\"tuple_list\"] for j in loaded_json_raw],\n",
    "                        }\n",
    "\n",
    "                        seed_scores = add_element_scores(loaded_json, task)\n",
    "\n",
    "                        scores.append(seed_scores)\n",
    "                scores_zeroshot[f\"{task}_{fs}_{dataset}\"] = calc_mean(scores)\n",
    "\n",
    "# WITH SELF-Consistency\n",
    "for dataset in DATASETS:\n",
    "    for task in TASKS:\n",
    "        for fs in [0, 10, 20, 30, 40, 50]:\n",
    "                all_example_data = []\n",
    "                for seed in range(N_SEEDS):\n",
    "                    with open(\n",
    "                        f\"../../zero-shot-absa-quad/generations/zeroshot/{task}_{dataset}_test_gemma3:27b_{seed}_label_{fs}.json\"\n",
    "                    ) as f:\n",
    "                        loaded_json_raw = json.load(f)\n",
    "\n",
    "                        loaded_json = {\n",
    "                            \"all_preds\": [j[\"pred_label\"] for j in loaded_json_raw],\n",
    "                            \"all_labels\": [j[\"tuple_list\"] for j in loaded_json_raw],\n",
    "                        }\n",
    "\n",
    "                        all_example_data.append(loaded_json)\n",
    "\n",
    "                all_labels = all_example_data[0][\"all_labels\"]\n",
    "                all_preds = [[] for _ in range(len(all_labels))]\n",
    "                for seed in range(0, N_SEEDS):\n",
    "                    for idx in range(len(all_labels)):\n",
    "                        all_preds[idx].append(all_example_data[seed][\"all_preds\"][idx])\n",
    "                        if seed == N_SEEDS - 1:\n",
    "                            all_preds[idx] = merge_aspect_lists(all_preds[idx])\n",
    "                            all_preds[idx] = [list(p) for p in all_preds[idx]]\n",
    "\n",
    "                loaded_json = {\n",
    "                    \"all_preds\": all_preds,\n",
    "                    \"all_labels\": all_labels,\n",
    "                }\n",
    "\n",
    "                scores = add_element_scores(loaded_json, task)\n",
    "                scores_zeroshot[f\"{task}_{fs}_{dataset}_sc\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aca7989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_train_qaie(task=\"tasd\", dataset=\"rest16\", fs=2):\n",
    "    path = f\"../../QAIE-ABSA-2025-adaption/01_augmentations/fs_examples/{task}/{dataset}/fs_{fs}/aug.txt\"\n",
    "    # count number of lines in the file\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        n_train = len(lines)\n",
    "    return n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72668d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tasd_0_rest15', 'tasd_10_rest15', 'tasd_20_rest15', 'tasd_30_rest15', 'tasd_40_rest15', 'tasd_50_rest15', 'asqp_0_rest15', 'asqp_10_rest15', 'asqp_20_rest15', 'asqp_30_rest15', 'asqp_40_rest15', 'asqp_50_rest15', 'tasd_0_rest16', 'tasd_10_rest16', 'tasd_20_rest16', 'tasd_30_rest16', 'tasd_40_rest16', 'tasd_50_rest16', 'asqp_0_rest16', 'asqp_10_rest16', 'asqp_20_rest16', 'asqp_30_rest16', 'asqp_40_rest16', 'asqp_50_rest16', 'tasd_0_flightabsa', 'tasd_10_flightabsa', 'tasd_20_flightabsa', 'tasd_30_flightabsa', 'tasd_40_flightabsa', 'tasd_50_flightabsa', 'asqp_0_flightabsa', 'asqp_10_flightabsa', 'asqp_20_flightabsa', 'asqp_30_flightabsa', 'asqp_40_flightabsa', 'asqp_50_flightabsa', 'tasd_0_coursera', 'tasd_10_coursera', 'tasd_20_coursera', 'tasd_30_coursera', 'tasd_40_coursera', 'tasd_50_coursera', 'asqp_0_coursera', 'asqp_10_coursera', 'asqp_20_coursera', 'asqp_30_coursera', 'asqp_40_coursera', 'asqp_50_coursera', 'tasd_0_hotels', 'tasd_10_hotels', 'tasd_20_hotels', 'tasd_30_hotels', 'tasd_40_hotels', 'tasd_50_hotels', 'asqp_0_hotels', 'asqp_10_hotels', 'asqp_20_hotels', 'asqp_30_hotels', 'asqp_40_hotels', 'asqp_50_hotels', 'tasd_0_rest15_sc', 'tasd_10_rest15_sc', 'tasd_20_rest15_sc', 'tasd_30_rest15_sc', 'tasd_40_rest15_sc', 'tasd_50_rest15_sc', 'asqp_0_rest15_sc', 'asqp_10_rest15_sc', 'asqp_20_rest15_sc', 'asqp_30_rest15_sc', 'asqp_40_rest15_sc', 'asqp_50_rest15_sc', 'tasd_0_rest16_sc', 'tasd_10_rest16_sc', 'tasd_20_rest16_sc', 'tasd_30_rest16_sc', 'tasd_40_rest16_sc', 'tasd_50_rest16_sc', 'asqp_0_rest16_sc', 'asqp_10_rest16_sc', 'asqp_20_rest16_sc', 'asqp_30_rest16_sc', 'asqp_40_rest16_sc', 'asqp_50_rest16_sc', 'tasd_0_flightabsa_sc', 'tasd_10_flightabsa_sc', 'tasd_20_flightabsa_sc', 'tasd_30_flightabsa_sc', 'tasd_40_flightabsa_sc', 'tasd_50_flightabsa_sc', 'asqp_0_flightabsa_sc', 'asqp_10_flightabsa_sc', 'asqp_20_flightabsa_sc', 'asqp_30_flightabsa_sc', 'asqp_40_flightabsa_sc', 'asqp_50_flightabsa_sc', 'tasd_0_coursera_sc', 'tasd_10_coursera_sc', 'tasd_20_coursera_sc', 'tasd_30_coursera_sc', 'tasd_40_coursera_sc', 'tasd_50_coursera_sc', 'asqp_0_coursera_sc', 'asqp_10_coursera_sc', 'asqp_20_coursera_sc', 'asqp_30_coursera_sc', 'asqp_40_coursera_sc', 'asqp_50_coursera_sc', 'tasd_0_hotels_sc', 'tasd_10_hotels_sc', 'tasd_20_hotels_sc', 'tasd_30_hotels_sc', 'tasd_40_hotels_sc', 'tasd_50_hotels_sc', 'asqp_0_hotels_sc', 'asqp_10_hotels_sc', 'asqp_20_hotels_sc', 'asqp_30_hotels_sc', 'asqp_40_hotels_sc', 'asqp_50_hotels_sc'])\n",
      "dict_keys(['dlo_10_tasd_rest15', 'dlo_50_tasd_rest15', 'dlo_full_tasd_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_tasd_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_tasd_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_rest15', 'dlo_10_asqp_rest15', 'dlo_50_asqp_rest15', 'dlo_full_asqp_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_asqp_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_asqp_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_rest15', 'dlo_10_tasd_rest16', 'dlo_50_tasd_rest16', 'dlo_full_tasd_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_tasd_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_tasd_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_rest16', 'dlo_10_asqp_rest16', 'dlo_50_asqp_rest16', 'dlo_full_asqp_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_asqp_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_asqp_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_rest16', 'dlo_10_tasd_flightabsa', 'dlo_50_tasd_flightabsa', 'dlo_full_tasd_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_tasd_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_tasd_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_flightabsa', 'dlo_10_asqp_flightabsa', 'dlo_50_asqp_flightabsa', 'dlo_full_asqp_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_asqp_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_asqp_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_flightabsa', 'dlo_10_tasd_coursera', 'dlo_50_tasd_coursera', 'dlo_full_tasd_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_tasd_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_tasd_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_coursera', 'dlo_10_asqp_coursera', 'dlo_50_asqp_coursera', 'dlo_full_asqp_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_asqp_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_asqp_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_coursera', 'dlo_10_tasd_hotels', 'dlo_50_tasd_hotels', 'dlo_full_tasd_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_tasd_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_tasd_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_hotels', 'dlo_10_asqp_hotels', 'dlo_50_asqp_hotels', 'dlo_full_asqp_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_10_asqp_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_50_asqp_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_hotels'])\n",
      "dict_keys(['dlo_full_tasd_0_rest15', 'dlo_full_tasd_10_rest15', 'dlo_full_tasd_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_rest15', 'dlo_full_asqp_0_rest15', 'dlo_full_asqp_10_rest15', 'dlo_full_asqp_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_rest15', 'dlo_full_tasd_0_rest16', 'dlo_full_tasd_10_rest16', 'dlo_full_tasd_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_rest16', 'dlo_full_asqp_0_rest16', 'dlo_full_asqp_10_rest16', 'dlo_full_asqp_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_rest16', 'dlo_full_tasd_0_flightabsa', 'dlo_full_tasd_10_flightabsa', 'dlo_full_tasd_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_flightabsa', 'dlo_full_asqp_0_flightabsa', 'dlo_full_asqp_10_flightabsa', 'dlo_full_asqp_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_flightabsa', 'dlo_full_tasd_0_coursera', 'dlo_full_tasd_10_coursera', 'dlo_full_tasd_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_coursera', 'dlo_full_asqp_0_coursera', 'dlo_full_asqp_10_coursera', 'dlo_full_asqp_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_coursera', 'dlo_full_tasd_0_hotels', 'dlo_full_tasd_10_hotels', 'dlo_full_tasd_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_0_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_tasd_50_hotels', 'dlo_full_asqp_0_hotels', 'dlo_full_asqp_10_hotels', 'dlo_full_asqp_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_0_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_full_asqp_50_hotels'])\n",
      "dict_keys(['qaie_tasd_10_rest15', 'qaie_tasd_50_rest15', 'qaie_asqp_10_rest15', 'qaie_asqp_50_rest15', 'qaie_tasd_10_rest16', 'qaie_tasd_50_rest16', 'qaie_asqp_10_rest16', 'qaie_asqp_50_rest16', 'qaie_tasd_10_flightabsa', 'qaie_tasd_50_flightabsa', 'qaie_asqp_10_flightabsa', 'qaie_asqp_50_flightabsa', 'qaie_tasd_10_coursera', 'qaie_tasd_50_coursera', 'qaie_asqp_10_coursera', 'qaie_asqp_50_coursera', 'qaie_tasd_10_hotels', 'qaie_tasd_50_hotels', 'qaie_asqp_10_hotels', 'qaie_asqp_50_hotels', 'dlo_eda_2_tasd_10_rest15', 'dlo_eda_5_tasd_10_rest15', 'dlo_eda_10_tasd_10_rest15', 'dlo_eda_2_tasd_50_rest15', 'dlo_eda_5_tasd_50_rest15', 'dlo_eda_10_tasd_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_50_rest15', 'dlo_eda_2_asqp_10_rest15', 'dlo_eda_5_asqp_10_rest15', 'dlo_eda_10_asqp_10_rest15', 'dlo_eda_2_asqp_50_rest15', 'dlo_eda_5_asqp_50_rest15', 'dlo_eda_10_asqp_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_10_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_50_rest15', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_50_rest15', 'dlo_eda_2_tasd_10_rest16', 'dlo_eda_5_tasd_10_rest16', 'dlo_eda_10_tasd_10_rest16', 'dlo_eda_2_tasd_50_rest16', 'dlo_eda_5_tasd_50_rest16', 'dlo_eda_10_tasd_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_50_rest16', 'dlo_eda_2_asqp_10_rest16', 'dlo_eda_5_asqp_10_rest16', 'dlo_eda_10_asqp_10_rest16', 'dlo_eda_2_asqp_50_rest16', 'dlo_eda_5_asqp_50_rest16', 'dlo_eda_10_asqp_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_10_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_50_rest16', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_50_rest16', 'dlo_eda_2_tasd_10_flightabsa', 'dlo_eda_5_tasd_10_flightabsa', 'dlo_eda_10_tasd_10_flightabsa', 'dlo_eda_2_tasd_50_flightabsa', 'dlo_eda_5_tasd_50_flightabsa', 'dlo_eda_10_tasd_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_50_flightabsa', 'dlo_eda_2_asqp_10_flightabsa', 'dlo_eda_5_asqp_10_flightabsa', 'dlo_eda_10_asqp_10_flightabsa', 'dlo_eda_2_asqp_50_flightabsa', 'dlo_eda_5_asqp_50_flightabsa', 'dlo_eda_10_asqp_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_10_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_50_flightabsa', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_50_flightabsa', 'dlo_eda_2_tasd_10_coursera', 'dlo_eda_5_tasd_10_coursera', 'dlo_eda_10_tasd_10_coursera', 'dlo_eda_2_tasd_50_coursera', 'dlo_eda_5_tasd_50_coursera', 'dlo_eda_10_tasd_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_50_coursera', 'dlo_eda_2_asqp_10_coursera', 'dlo_eda_5_asqp_10_coursera', 'dlo_eda_10_asqp_10_coursera', 'dlo_eda_2_asqp_50_coursera', 'dlo_eda_5_asqp_50_coursera', 'dlo_eda_10_asqp_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_10_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_50_coursera', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_50_coursera', 'dlo_eda_2_tasd_10_hotels', 'dlo_eda_5_tasd_10_hotels', 'dlo_eda_10_tasd_10_hotels', 'dlo_eda_2_tasd_50_hotels', 'dlo_eda_5_tasd_50_hotels', 'dlo_eda_10_tasd_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_tasd_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_tasd_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_tasd_50_hotels', 'dlo_eda_2_asqp_10_hotels', 'dlo_eda_5_asqp_10_hotels', 'dlo_eda_10_asqp_10_hotels', 'dlo_eda_2_asqp_50_hotels', 'dlo_eda_5_asqp_50_hotels', 'dlo_eda_10_asqp_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_10_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_2_asqp_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_5_asqp_50_hotels', 'llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit_eda_10_asqp_50_hotels'])\n"
     ]
    }
   ],
   "source": [
    "print(scores_zeroshot.keys())\n",
    "print(scores_00_baseline.keys())\n",
    "print(scores_llm_ann_train.keys())\n",
    "print(scores_traditional_aug.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cf4baa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 27 27\n",
      "27 27 27\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\# Annotated examples</th>\n",
       "      <th>Approach</th>\n",
       "      <th>\\# Train</th>\n",
       "      <th>rest15_tasd_f1</th>\n",
       "      <th>rest15_asqp_f1</th>\n",
       "      <th>rest16_tasd_f1</th>\n",
       "      <th>rest16_asqp_f1</th>\n",
       "      <th>flightabsa_tasd_f1</th>\n",
       "      <th>flightabsa_asqp_f1</th>\n",
       "      <th>coursera_tasd_f1</th>\n",
       "      <th>coursera_asqp_f1</th>\n",
       "      <th>hotels_tasd_f1</th>\n",
       "      <th>hotels_asqp_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit</td>\n",
       "      <td>10</td>\n",
       "      <td>15.84</td>\n",
       "      <td>4.37</td>\n",
       "      <td>13.59</td>\n",
       "      <td>5.18</td>\n",
       "      <td>16.07</td>\n",
       "      <td>4.87</td>\n",
       "      <td>22.93</td>\n",
       "      <td>4.47</td>\n",
       "      <td>18.07</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>DLO</td>\n",
       "      <td>10</td>\n",
       "      <td>31.79</td>\n",
       "      <td>15.38</td>\n",
       "      <td>36.26</td>\n",
       "      <td>15.63</td>\n",
       "      <td>36.62</td>\n",
       "      <td>18.50</td>\n",
       "      <td>21.41</td>\n",
       "      <td>8.20</td>\n",
       "      <td>31.31</td>\n",
       "      <td>16.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>LLMA \\textbackslash w llm_Meta-Llama-3.1-8B-In...</td>\n",
       "      <td>full</td>\n",
       "      <td>49.23</td>\n",
       "      <td>37.19</td>\n",
       "      <td>62.37</td>\n",
       "      <td>46.20</td>\n",
       "      <td>61.40</td>\n",
       "      <td>46.47</td>\n",
       "      <td>39.22</td>\n",
       "      <td>23.37</td>\n",
       "      <td>55.27</td>\n",
       "      <td>31.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>LLMA \\textbackslash w DLO</td>\n",
       "      <td>full</td>\n",
       "      <td>53.22</td>\n",
       "      <td>37.47</td>\n",
       "      <td>63.02</td>\n",
       "      <td>43.84</td>\n",
       "      <td>61.73</td>\n",
       "      <td>46.01</td>\n",
       "      <td>38.46</td>\n",
       "      <td>23.41</td>\n",
       "      <td>55.07</td>\n",
       "      <td>32.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...</td>\n",
       "      <td>30</td>\n",
       "      <td>24.58</td>\n",
       "      <td>6.74</td>\n",
       "      <td>16.97</td>\n",
       "      <td>9.27</td>\n",
       "      <td>21.18</td>\n",
       "      <td>12.13</td>\n",
       "      <td>28.14</td>\n",
       "      <td>9.75</td>\n",
       "      <td>22.73</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...</td>\n",
       "      <td>60</td>\n",
       "      <td>26.65</td>\n",
       "      <td>8.68</td>\n",
       "      <td>18.06</td>\n",
       "      <td>10.70</td>\n",
       "      <td>21.66</td>\n",
       "      <td>12.40</td>\n",
       "      <td>28.25</td>\n",
       "      <td>11.73</td>\n",
       "      <td>23.95</td>\n",
       "      <td>7.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...</td>\n",
       "      <td>110</td>\n",
       "      <td>29.31</td>\n",
       "      <td>9.22</td>\n",
       "      <td>19.16</td>\n",
       "      <td>11.24</td>\n",
       "      <td>22.66</td>\n",
       "      <td>11.77</td>\n",
       "      <td>28.48</td>\n",
       "      <td>13.00</td>\n",
       "      <td>24.57</td>\n",
       "      <td>7.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>EDA \\textbackslash w DLO</td>\n",
       "      <td>30</td>\n",
       "      <td>35.96</td>\n",
       "      <td>16.24</td>\n",
       "      <td>47.23</td>\n",
       "      <td>19.62</td>\n",
       "      <td>35.36</td>\n",
       "      <td>19.21</td>\n",
       "      <td>22.76</td>\n",
       "      <td>8.90</td>\n",
       "      <td>28.15</td>\n",
       "      <td>17.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>EDA \\textbackslash w DLO</td>\n",
       "      <td>60</td>\n",
       "      <td>39.59</td>\n",
       "      <td>16.85</td>\n",
       "      <td>41.92</td>\n",
       "      <td>21.59</td>\n",
       "      <td>32.69</td>\n",
       "      <td>22.23</td>\n",
       "      <td>30.03</td>\n",
       "      <td>14.18</td>\n",
       "      <td>33.52</td>\n",
       "      <td>19.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>EDA \\textbackslash w DLO</td>\n",
       "      <td>110</td>\n",
       "      <td>40.10</td>\n",
       "      <td>13.69</td>\n",
       "      <td>40.20</td>\n",
       "      <td>21.22</td>\n",
       "      <td>31.38</td>\n",
       "      <td>21.35</td>\n",
       "      <td>31.79</td>\n",
       "      <td>12.72</td>\n",
       "      <td>31.15</td>\n",
       "      <td>18.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>QAIE</td>\n",
       "      <td>26.4 / 45.2</td>\n",
       "      <td>20.11</td>\n",
       "      <td>9.96</td>\n",
       "      <td>12.37</td>\n",
       "      <td>14.78</td>\n",
       "      <td>17.76</td>\n",
       "      <td>15.38</td>\n",
       "      <td>25.18</td>\n",
       "      <td>17.54</td>\n",
       "      <td>21.69</td>\n",
       "      <td>9.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit</td>\n",
       "      <td>50</td>\n",
       "      <td>39.54</td>\n",
       "      <td>26.63</td>\n",
       "      <td>43.95</td>\n",
       "      <td>29.57</td>\n",
       "      <td>42.92</td>\n",
       "      <td>28.74</td>\n",
       "      <td>36.04</td>\n",
       "      <td>19.08</td>\n",
       "      <td>44.72</td>\n",
       "      <td>27.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>DLO</td>\n",
       "      <td>50</td>\n",
       "      <td>49.16</td>\n",
       "      <td>32.00</td>\n",
       "      <td>50.72</td>\n",
       "      <td>35.66</td>\n",
       "      <td>49.10</td>\n",
       "      <td>32.71</td>\n",
       "      <td>34.31</td>\n",
       "      <td>20.60</td>\n",
       "      <td>42.48</td>\n",
       "      <td>27.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>50</td>\n",
       "      <td>LLMA \\textbackslash w llm_Meta-Llama-3.1-8B-In...</td>\n",
       "      <td>full</td>\n",
       "      <td>58.40</td>\n",
       "      <td>40.38</td>\n",
       "      <td>62.03</td>\n",
       "      <td>49.85</td>\n",
       "      <td>62.57</td>\n",
       "      <td>48.98</td>\n",
       "      <td>44.39</td>\n",
       "      <td>25.69</td>\n",
       "      <td>61.43</td>\n",
       "      <td>44.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50</td>\n",
       "      <td>LLMA \\textbackslash w DLO</td>\n",
       "      <td>full</td>\n",
       "      <td>61.45</td>\n",
       "      <td>39.01</td>\n",
       "      <td>64.72</td>\n",
       "      <td>50.02</td>\n",
       "      <td>62.79</td>\n",
       "      <td>47.25</td>\n",
       "      <td>44.43</td>\n",
       "      <td>24.40</td>\n",
       "      <td>61.15</td>\n",
       "      <td>43.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...</td>\n",
       "      <td>150</td>\n",
       "      <td>43.86</td>\n",
       "      <td>30.41</td>\n",
       "      <td>46.79</td>\n",
       "      <td>33.52</td>\n",
       "      <td>45.60</td>\n",
       "      <td>34.84</td>\n",
       "      <td>37.81</td>\n",
       "      <td>23.53</td>\n",
       "      <td>45.89</td>\n",
       "      <td>31.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>50</td>\n",
       "      <td>EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...</td>\n",
       "      <td>300</td>\n",
       "      <td>44.76</td>\n",
       "      <td>30.90</td>\n",
       "      <td>46.99</td>\n",
       "      <td>34.42</td>\n",
       "      <td>46.48</td>\n",
       "      <td>36.37</td>\n",
       "      <td>38.32</td>\n",
       "      <td>23.53</td>\n",
       "      <td>45.20</td>\n",
       "      <td>32.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>50</td>\n",
       "      <td>EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...</td>\n",
       "      <td>550</td>\n",
       "      <td>44.14</td>\n",
       "      <td>30.93</td>\n",
       "      <td>45.49</td>\n",
       "      <td>34.59</td>\n",
       "      <td>45.64</td>\n",
       "      <td>36.73</td>\n",
       "      <td>37.89</td>\n",
       "      <td>24.57</td>\n",
       "      <td>45.13</td>\n",
       "      <td>32.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50</td>\n",
       "      <td>EDA \\textbackslash w DLO</td>\n",
       "      <td>150</td>\n",
       "      <td>49.59</td>\n",
       "      <td>31.90</td>\n",
       "      <td>52.36</td>\n",
       "      <td>37.89</td>\n",
       "      <td>50.82</td>\n",
       "      <td>35.38</td>\n",
       "      <td>37.07</td>\n",
       "      <td>20.30</td>\n",
       "      <td>50.81</td>\n",
       "      <td>32.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50</td>\n",
       "      <td>EDA \\textbackslash w DLO</td>\n",
       "      <td>300</td>\n",
       "      <td>50.30</td>\n",
       "      <td>31.54</td>\n",
       "      <td>52.87</td>\n",
       "      <td>39.22</td>\n",
       "      <td>49.59</td>\n",
       "      <td>39.27</td>\n",
       "      <td>38.99</td>\n",
       "      <td>21.85</td>\n",
       "      <td>51.76</td>\n",
       "      <td>33.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50</td>\n",
       "      <td>EDA \\textbackslash w DLO</td>\n",
       "      <td>550</td>\n",
       "      <td>50.73</td>\n",
       "      <td>30.70</td>\n",
       "      <td>53.46</td>\n",
       "      <td>39.36</td>\n",
       "      <td>50.68</td>\n",
       "      <td>39.40</td>\n",
       "      <td>39.85</td>\n",
       "      <td>21.61</td>\n",
       "      <td>50.81</td>\n",
       "      <td>32.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>QAIE</td>\n",
       "      <td>141.8 / 248.8</td>\n",
       "      <td>45.01</td>\n",
       "      <td>33.87</td>\n",
       "      <td>45.09</td>\n",
       "      <td>35.21</td>\n",
       "      <td>48.44</td>\n",
       "      <td>33.98</td>\n",
       "      <td>36.23</td>\n",
       "      <td>22.45</td>\n",
       "      <td>50.51</td>\n",
       "      <td>35.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>full</td>\n",
       "      <td>llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit</td>\n",
       "      <td>full</td>\n",
       "      <td>62.95</td>\n",
       "      <td>48.18</td>\n",
       "      <td>71.79</td>\n",
       "      <td>59.79</td>\n",
       "      <td>68.95</td>\n",
       "      <td>58.33</td>\n",
       "      <td>52.58</td>\n",
       "      <td>32.54</td>\n",
       "      <td>68.56</td>\n",
       "      <td>55.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>full</td>\n",
       "      <td>DLO</td>\n",
       "      <td>full</td>\n",
       "      <td>68.07</td>\n",
       "      <td>49.50</td>\n",
       "      <td>72.07</td>\n",
       "      <td>58.19</td>\n",
       "      <td>69.19</td>\n",
       "      <td>56.35</td>\n",
       "      <td>49.93</td>\n",
       "      <td>30.96</td>\n",
       "      <td>66.64</td>\n",
       "      <td>53.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Gemma-3-27B (Prompting)</td>\n",
       "      <td>0</td>\n",
       "      <td>30.36</td>\n",
       "      <td>24.73</td>\n",
       "      <td>45.51</td>\n",
       "      <td>28.96</td>\n",
       "      <td>51.81</td>\n",
       "      <td>42.37</td>\n",
       "      <td>29.50</td>\n",
       "      <td>13.36</td>\n",
       "      <td>38.97</td>\n",
       "      <td>23.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>Gemma-3-27B (Prompting)</td>\n",
       "      <td>10</td>\n",
       "      <td>54.47</td>\n",
       "      <td>39.95</td>\n",
       "      <td>66.75</td>\n",
       "      <td>46.23</td>\n",
       "      <td>60.36</td>\n",
       "      <td>45.24</td>\n",
       "      <td>41.69</td>\n",
       "      <td>22.31</td>\n",
       "      <td>56.51</td>\n",
       "      <td>31.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50</td>\n",
       "      <td>Gemma-3-27B (Prompting)</td>\n",
       "      <td>50</td>\n",
       "      <td>62.12</td>\n",
       "      <td>41.74</td>\n",
       "      <td>68.53</td>\n",
       "      <td>51.10</td>\n",
       "      <td>64.60</td>\n",
       "      <td>48.37</td>\n",
       "      <td>44.80</td>\n",
       "      <td>25.86</td>\n",
       "      <td>62.97</td>\n",
       "      <td>43.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   \\# Annotated examples                                           Approach  \\\n",
       "0                     10            llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit   \n",
       "1                     10                                                DLO   \n",
       "2                     10  LLMA \\textbackslash w llm_Meta-Llama-3.1-8B-In...   \n",
       "3                     10                          LLMA \\textbackslash w DLO   \n",
       "4                     10  EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...   \n",
       "5                     10  EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...   \n",
       "6                     10  EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...   \n",
       "7                     10                           EDA \\textbackslash w DLO   \n",
       "8                     10                           EDA \\textbackslash w DLO   \n",
       "9                     10                           EDA \\textbackslash w DLO   \n",
       "10                    10                                               QAIE   \n",
       "11                    50            llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit   \n",
       "12                    50                                                DLO   \n",
       "13                    50  LLMA \\textbackslash w llm_Meta-Llama-3.1-8B-In...   \n",
       "14                    50                          LLMA \\textbackslash w DLO   \n",
       "15                    50  EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...   \n",
       "16                    50  EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...   \n",
       "17                    50  EDA \\textbackslash w llm_Meta-Llama-3.1-8B-Ins...   \n",
       "18                    50                           EDA \\textbackslash w DLO   \n",
       "19                    50                           EDA \\textbackslash w DLO   \n",
       "20                    50                           EDA \\textbackslash w DLO   \n",
       "21                    50                                               QAIE   \n",
       "22                  full            llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit   \n",
       "23                  full                                                DLO   \n",
       "24                     0                            Gemma-3-27B (Prompting)   \n",
       "25                    10                            Gemma-3-27B (Prompting)   \n",
       "26                    50                            Gemma-3-27B (Prompting)   \n",
       "\n",
       "         \\# Train rest15_tasd_f1 rest15_asqp_f1 rest16_tasd_f1 rest16_asqp_f1  \\\n",
       "0              10          15.84           4.37          13.59           5.18   \n",
       "1              10          31.79          15.38          36.26          15.63   \n",
       "2            full          49.23          37.19          62.37          46.20   \n",
       "3            full          53.22          37.47          63.02          43.84   \n",
       "4              30          24.58           6.74          16.97           9.27   \n",
       "5              60          26.65           8.68          18.06          10.70   \n",
       "6             110          29.31           9.22          19.16          11.24   \n",
       "7              30          35.96          16.24          47.23          19.62   \n",
       "8              60          39.59          16.85          41.92          21.59   \n",
       "9             110          40.10          13.69          40.20          21.22   \n",
       "10    26.4 / 45.2          20.11           9.96          12.37          14.78   \n",
       "11             50          39.54          26.63          43.95          29.57   \n",
       "12             50          49.16          32.00          50.72          35.66   \n",
       "13           full          58.40          40.38          62.03          49.85   \n",
       "14           full          61.45          39.01          64.72          50.02   \n",
       "15            150          43.86          30.41          46.79          33.52   \n",
       "16            300          44.76          30.90          46.99          34.42   \n",
       "17            550          44.14          30.93          45.49          34.59   \n",
       "18            150          49.59          31.90          52.36          37.89   \n",
       "19            300          50.30          31.54          52.87          39.22   \n",
       "20            550          50.73          30.70          53.46          39.36   \n",
       "21  141.8 / 248.8          45.01          33.87          45.09          35.21   \n",
       "22           full          62.95          48.18          71.79          59.79   \n",
       "23           full          68.07          49.50          72.07          58.19   \n",
       "24              0          30.36          24.73          45.51          28.96   \n",
       "25             10          54.47          39.95          66.75          46.23   \n",
       "26             50          62.12          41.74          68.53          51.10   \n",
       "\n",
       "   flightabsa_tasd_f1 flightabsa_asqp_f1 coursera_tasd_f1 coursera_asqp_f1  \\\n",
       "0               16.07               4.87            22.93             4.47   \n",
       "1               36.62              18.50            21.41             8.20   \n",
       "2               61.40              46.47            39.22            23.37   \n",
       "3               61.73              46.01            38.46            23.41   \n",
       "4               21.18              12.13            28.14             9.75   \n",
       "5               21.66              12.40            28.25            11.73   \n",
       "6               22.66              11.77            28.48            13.00   \n",
       "7               35.36              19.21            22.76             8.90   \n",
       "8               32.69              22.23            30.03            14.18   \n",
       "9               31.38              21.35            31.79            12.72   \n",
       "10              17.76              15.38            25.18            17.54   \n",
       "11              42.92              28.74            36.04            19.08   \n",
       "12              49.10              32.71            34.31            20.60   \n",
       "13              62.57              48.98            44.39            25.69   \n",
       "14              62.79              47.25            44.43            24.40   \n",
       "15              45.60              34.84            37.81            23.53   \n",
       "16              46.48              36.37            38.32            23.53   \n",
       "17              45.64              36.73            37.89            24.57   \n",
       "18              50.82              35.38            37.07            20.30   \n",
       "19              49.59              39.27            38.99            21.85   \n",
       "20              50.68              39.40            39.85            21.61   \n",
       "21              48.44              33.98            36.23            22.45   \n",
       "22              68.95              58.33            52.58            32.54   \n",
       "23              69.19              56.35            49.93            30.96   \n",
       "24              51.81              42.37            29.50            13.36   \n",
       "25              60.36              45.24            41.69            22.31   \n",
       "26              64.60              48.37            44.80            25.86   \n",
       "\n",
       "   hotels_tasd_f1 hotels_asqp_f1  \n",
       "0           18.07           3.53  \n",
       "1           31.31          16.55  \n",
       "2           55.27          31.44  \n",
       "3           55.07          32.42  \n",
       "4           22.73           5.98  \n",
       "5           23.95           7.51  \n",
       "6           24.57           7.44  \n",
       "7           28.15          17.32  \n",
       "8           33.52          19.29  \n",
       "9           31.15          18.49  \n",
       "10          21.69           9.86  \n",
       "11          44.72          27.20  \n",
       "12          42.48          27.76  \n",
       "13          61.43          44.24  \n",
       "14          61.15          43.07  \n",
       "15          45.89          31.39  \n",
       "16          45.20          32.55  \n",
       "17          45.13          32.51  \n",
       "18          50.81          32.03  \n",
       "19          51.76          33.38  \n",
       "20          50.81          32.85  \n",
       "21          50.51          35.95  \n",
       "22          68.56          55.45  \n",
       "23          66.64          53.92  \n",
       "24          38.97          23.02  \n",
       "25          56.51          31.41  \n",
       "26          62.97          43.83  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FT_APPROACHES = [\"llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit\", \"DLO\"]  # , \"Llama-3-8B FT\"]\n",
    "FT_ENCODING = {\"FT-LLama-3-8B\": \"llm_Meta-Llama-3.1-8B-Instruct-bnb-4bit\", \"DLO\": \"dlo\", \"Llama-3-8B FT\": \"llama\"}\n",
    "FT_ENCODING_REVERSE = {v: k for k, v in FT_ENCODING.items()}\n",
    "\n",
    "N_TRAIN_EDA = [2, 5, 10]\n",
    "N_SHOTS = [10, 50]\n",
    "\n",
    "\n",
    "def create_f1_plot(tasks=[\"tasd\"], metrics=[\"f1\"]):\n",
    "\n",
    "    n_annotated_example_column = (\n",
    "        [10]\n",
    "        * (len(FT_APPROACHES) * 2 + len(N_TRAIN_EDA) * len(FT_APPROACHES) + 1)\n",
    "        + [50]\n",
    "        * (len(FT_APPROACHES) * 2 + len(N_TRAIN_EDA) * len(FT_APPROACHES) + 1) + [\"full\" for _ in range(len(FT_APPROACHES))] + [0, 10, 50]\n",
    "    )\n",
    "\n",
    "    approaches_column = (\n",
    "        FT_APPROACHES\n",
    "        + [\"LLMA \\\\textbackslash w \" + approach for approach in FT_APPROACHES]\n",
    "        + [\n",
    "            \"EDA \\\\textbackslash w \" + approach\n",
    "            for approach in FT_APPROACHES\n",
    "            for _ in range(len(N_TRAIN_EDA))\n",
    "        ]\n",
    "        + [\"QAIE\"]\n",
    "    ) * 2 + [f\"{approach}\" for approach in FT_APPROACHES] + [\"Gemma-3-27B (Prompting)\" for _ in range(len(N_SHOTS) + 1)]\n",
    "\n",
    "    n_train_column = []\n",
    "\n",
    "    for fs in N_SHOTS:\n",
    "        n_train_column += (\n",
    "            [fs] * len(FT_APPROACHES)\n",
    "            + [\"full\"] * len(FT_APPROACHES)\n",
    "            + [fs + n * fs for _ in FT_APPROACHES for n in N_TRAIN_EDA]\n",
    "            + [\n",
    "                \" / \".join(\n",
    "                    [\n",
    "                        str(\n",
    "                            np.round(\n",
    "                                np.mean(\n",
    "                                    [\n",
    "                                        get_n_train_qaie(task=task, dataset=ds, fs=fs)\n",
    "                                        for ds in DATASETS\n",
    "                                    ]\n",
    "                                ),\n",
    "                                1,\n",
    "                            )\n",
    "                        )\n",
    "                        for task in tasks\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        ) \n",
    "    \n",
    "    n_train_column += [\"full\" for _ in FT_APPROACHES]\n",
    "    n_train_column += [0, 10, 50]\n",
    "\n",
    "    performance_scores = {\n",
    "        dataset: { task: {metric: [] for metric in metrics} for task in tasks }\n",
    "        for dataset in DATASETS \n",
    "    }\n",
    "    \n",
    "    print(len(n_annotated_example_column), len(approaches_column), len(n_train_column))\n",
    "    \n",
    "\n",
    "    for dataset in DATASETS:\n",
    "        for task in tasks:\n",
    "            for metric in metrics:\n",
    "\n",
    "                for fs in [10, 50]:\n",
    "                    performance_scores[dataset][task][metric].extend(\n",
    "                        [\n",
    "                            scores_00_baseline[f\"{method}_{fs}_{task}_{dataset}\"][\n",
    "                                metric\n",
    "                            ]\n",
    "                            for method in METHODS\n",
    "                        ]\n",
    "                    )\n",
    "                    performance_scores[dataset][task][metric].extend(\n",
    "                        [\n",
    "                            scores_llm_ann_train[\n",
    "                                f\"{method}_full_{task}_{fs}_{dataset}\"\n",
    "                            ][metric]\n",
    "                            for method in METHODS\n",
    "                        ]\n",
    "                    )\n",
    "                    performance_scores[dataset][task][metric].extend(\n",
    "                        [\n",
    "                            scores_traditional_aug[\n",
    "                                f\"{method}_eda_{n_train}_{task}_{fs}_{dataset}\"\n",
    "                            ][metric]\n",
    "                            for method in METHODS\n",
    "                            for n_train in N_TRAIN_EDA\n",
    "                        ]\n",
    "                    )\n",
    "                    performance_scores[dataset][task][metric].append(\n",
    "                        scores_traditional_aug[f\"qaie_{task}_{fs}_{dataset}\"][metric]\n",
    "                    )\n",
    "                    \n",
    "                for method in METHODS:\n",
    "                   performance_scores[dataset][task][metric].append(\n",
    "                        scores_00_baseline[f\"{method}_full_{task}_{dataset}\"][metric]\n",
    "                   )\n",
    "                for fs in [0, 10, 50]:\n",
    "                   performance_scores[dataset][task][metric].append(\n",
    "                    scores_zeroshot[f\"{task}_{fs}_{dataset}_sc\"][metric]\n",
    "                   )\n",
    "\n",
    "    # Create DataFrame\n",
    "    print(len(n_annotated_example_column), len(approaches_column), len(n_train_column))\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"\\# Annotated examples\": n_annotated_example_column,\n",
    "            \"Approach\": approaches_column,\n",
    "            \"\\# Train\": n_train_column,\n",
    "            **{\n",
    "                f\"{dataset}_{task}_{metric}\": performance_scores[\n",
    "                    dataset\n",
    "                ][task][metric]\n",
    "                for dataset in DATASETS\n",
    "                for task in tasks\n",
    "                for metric in metrics\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    bolden_column = [\n",
    "        f\"{dataset}_{task}_{metric}\"\n",
    "        for dataset in DATASETS\n",
    "        for task in tasks\n",
    "        for metric in metrics\n",
    "    ]\n",
    "\n",
    "    df = round_numbers(df, bolden_column, n_rest=2)\n",
    "    # df = minimize(df, [\"\\# Annotated examples\"])\n",
    "    # df = bolden(df, bolden_column, \"\\# Annotated examples\")\n",
    "\n",
    "    # for i in range(len(df.columns)):\n",
    "    #     print(df.columns[i])\n",
    "    #     df.rename(columns={df.columns[i]: \"\\\\textbf{\" + df.columns[i] + \"}\"}, inplace=True)\n",
    "\n",
    "    column_format = (\n",
    "        \"p{5cm}p{6cm}\" + \"\".join(\"r\" for i in range(len(df.columns) - 3)) + \"r\"\n",
    "    )\n",
    "\n",
    "    latex_tabelle = df.to_latex(index=False, escape=False, column_format=column_format)\n",
    "\n",
    "    # latex_tabelle = insert_line(latex_tabelle, 5, \"black\", 1, 8, double_line=False)\n",
    "    # latex_tabelle = insert_line(latex_tabelle, 2, \"gray!80\", 2, 8, double_line=False)\n",
    "    # display_table(latex_tabelle)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "table_out = create_f1_plot(\n",
    "    tasks=[\"tasd\", \"asqp\"], metrics=[\"f1\"]\n",
    ")\n",
    "table_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9783d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\x08egin{table*}[t]\\n\\\\centering\\n\\\\small\\n\\\\setlength{\\tabcolsep}{4pt}\\n\\resizebox{2.0\\\\columnwidth}{!}{%\\n\\x08egin{tabular}{@{}llccccccccccc@{}}\\n\\toprule\\n\\\\multirow{2}{*}{\\textbf{Approach}} & \\\\multicolumn{2}{c}{\\textbf{\\\\# Train}} & \\\\multicolumn{2}{c}{\\textbf{Rest15}} & \\\\multicolumn{2}{c}{\\textbf{Rest16}} & \\\\multicolumn{2}{c}{\\textbf{Coursera}} & \\\\multicolumn{2}{c}{\\textbf{Hotels}} & \\\\multicolumn{2}{c}{\\textbf{FlightABSA}} \\\\\\n\\\\cmidrule(lr){2-3} \\\\cmidrule(lr){4-5} \\\\cmidrule(lr){6-7} \\\\cmidrule(lr){8-9} \\\\cmidrule(lr){10-11} \\\\cmidrule(l){12-13}\\n& TASD & ASQP & TASD & ASQP & TASD & ASQP & TASD & ASQP & TASD & ASQP & TASD & ASQP  \\\\\\n\\\\midrule\\nGemma-3-27B (Prompting) & 0 & 0 & 30.36 & 24.73 & 45.51 & 31.62 & 34.12 & 13.36 & 38.97 & 23.02 & 41.25 & 33.18 \\\\\\nGemma-3-27B (Prompting) & 10 & 10 & 54.47 & 39.95 & 66.75 & 48.24 & 42.16 & 22.31 & 56.51 & 31.41 & 58.73 & 42.29 \\\\\\n\\\\midrule\\n\\\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 10 examples or all training examples annotated by Gemma-3-27B, LLM-as-a-annotator (LLMA)}} \\\\\\n\\\\midrule\\nParaphrase & 10 & 10 & 8.75 & 1.32 & 6.66 & 3.27 & 19.35 & 4.75 & 14.91 & 2.63 & 19.82 & 7.45 \\\\\\nDLO & 10 & 10 & 15.84 & 4.37 & 13.59 & 6.85 & 25.61 & 4.47 & 18.07 & 3.53 & 22.14 & 9.26 \\\\\\nLLMA w/ Paraphrase & full & full & 49.09 & 35.04 & 62.74 & 45.32 & 40.25 & 22.47 & 55.69 & 32.83 & 54.31 & 41.56 \\\\\\nLLMA w/ DLO & full & full & 49.23 & 37.19 & 62.37 & 44.98 & 41.37 & 23.37 & 55.27 & 31.44 & 55.09 & 43.87 \\\\\\n\\\\midrule\\n\\\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 10 examples + EDA- or QAIE-augmented examples}} \\\\\\n\\\\midrule\\nEDA w/ Paraphrase & 30 & 30 & 13.15 & 1.74 & 7.67 & 3.95 & 22.48 & 6.61 & 22.05 & 3.47 & 24.37 & 10.23 \\\\\\nEDA w/ Paraphrase & 60 & 60 & 23.20 & 10.42 & 16.35 & 8.76 & 27.59 & 9.47 & 25.85 & 6.15 & 28.91 & 15.74 \\\\\\nEDA w/ Paraphrase & 110 & 110 & 26.07 & 10.52 & 16.82 & 9.05 & 30.17 & 10.35 & 24.95 & 5.84 & 29.85 & 16.47 \\\\\\nEDA w/ DLO & 30 & 30 & 24.58 & 6.74 & 16.97 & 8.33 & 29.85 & 9.75 & 22.73 & 5.98 & 28.44 & 13.92 \\\\\\nEDA w/ DLO & 60 & 60 & 26.65 & 8.68 & 18.06 & 9.14 & 31.36 & 11.73 & 23.95 & 7.51 & 30.79 & 17.36 \\\\\\nEDA w/ DLO & 110 & 110 & 29.31 & 9.22 & 19.16 & 10.43 & 32.87 & 13.00 & 24.57 & 7.44 & 32.68 & 18.25 \\\\\\nQAIE & 26.4 & 45.2 & 20.11 & 9.96 & 12.37 & 7.15 & 28.40 & 17.54 & 21.69 & 9.86 & 25.83 & 12.51 \\\\\\n\\\\midrule\\n\\\\midrule\\nGemma-3-27B (Prompting) & 50 & 50 & 62.12 & 41.74 & 68.53 & 49.87 & 45.87 & 25.86 & 62.97 & 43.83 & 67.45 & 53.21 \\\\\\n\\\\midrule\\n\\\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 50 examples or all training examples annotated by Gemma-3-27B, LLM-as-a-annotator (LLMA)}} \\\\\\n\\\\midrule\\nParaphrase & 50 & 50 & 36.92 & 25.55 & 35.87 & 24.12 & 35.78 & 19.38 & 40.10 & 23.09 & 39.65 & 30.76 \\\\\\nDLO & 50 & 50 & 39.54 & 26.63 & 43.95 & 28.44 & 38.46 & 19.08 & 44.72 & 27.20 & 43.88 & 33.47 \\\\\\nLLMA w/ Paraphrase & full & full & 56.21 & 37.61 & 62.20 & 44.73 & 43.46 & 25.71 & 60.58 & 43.19 & 62.94 & 51.85 \\\\\\nLLMA w/ DLO & full & full & 58.40 & 40.38 & 62.03 & 45.02 & 44.52 & 25.69 & 61.43 & 44.24 & 64.76 & 54.37 \\\\\\n\\\\midrule\\n\\\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 50 examples + EDA- or QAIE-augmented examples}} \\\\\\n\\\\midrule\\nEDA w/ Paraphrase & 150 & 150 & 42.70 & 27.55 & 41.59 & 27.85 & 39.73 & 19.90 & 43.44 & 27.96 & 45.28 & 34.60 \\\\\\nEDA w/ Paraphrase & 300 & 300 & 42.29 & 28.96 & 42.57 & 28.62 & 40.25 & 21.08 & 44.67 & 28.04 & 46.12 & 35.97 \\\\\\nEDA w/ Paraphrase & 550 & 550 & 41.83 & 28.70 & 43.01 & 28.77 & 40.09 & 21.37 & 42.70 & 27.97 & 45.76 & 35.42 \\\\\\nEDA w/ DLO & 150 & 150 & 43.86 & 30.41 & 46.79 & 31.43 & 40.94 & 23.53 & 45.89 & 31.39 & 48.35 & 38.72 \\\\\\nEDA w/ DLO & 300 & 300 & 44.76 & 30.90 & 46.99 & 31.99 & 41.53 & 23.53 & 45.20 & 32.55 & 49.03 & 39.87 \\\\\\nEDA w/ DLO & 550 & 550 & 44.14 & 30.93 & 45.49 & 31.25 & 41.34 & 24.57 & 45.13 & 32.51 & 48.66 & 39.45 \\\\\\nQAIE & 141.8 & 248.8 & 45.01 & 33.87 & 45.09 & 30.79 & 41.05 & 22.45 & 50.51 & 35.95 & 50.37 & 41.92 \\\\\\n\\x08ottomrule\\n\\\\end{tabular}\\n}\\n\\\\caption{Performance comparison of different approaches on aspect-based sentiment analysis tasks across five datasets. Results are reported for two main metrics: TASD F1 (Target Aspect Sentiment Detection) and ASQP F1 (Aspect Sentiment Quad Prediction).}\\n\\\\label{tab:results}\\n\\\\end{table*}\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\\begin{table*}[t]\n",
    "\\centering\n",
    "\\small\n",
    "\\setlength{\\tabcolsep}{4pt}\n",
    "\\resizebox{2.0\\columnwidth}{!}{%\n",
    "\\begin{tabular}{@{}llccccccccccc@{}}\n",
    "\\toprule\n",
    "\\multirow{2}{*}{\\textbf{Approach}} & \\multicolumn{2}{c}{\\textbf{\\# Train}} & \\multicolumn{2}{c}{\\textbf{Rest15}} & \\multicolumn{2}{c}{\\textbf{Rest16}} & \\multicolumn{2}{c}{\\textbf{Coursera}} & \\multicolumn{2}{c}{\\textbf{Hotels}} & \\multicolumn{2}{c}{\\textbf{FlightABSA}} \\\\\n",
    "\\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(l){12-13}\n",
    "& TASD & ASQP & TASD & ASQP & TASD & ASQP & TASD & ASQP & TASD & ASQP & TASD & ASQP  \\\\\n",
    "\\midrule\n",
    "Gemma-3-27B (Prompting) & 0 & 0 & 30.36 & 24.73 & 45.51 & 31.62 & 34.12 & 13.36 & 38.97 & 23.02 & 41.25 & 33.18 \\\\\n",
    "Gemma-3-27B (Prompting) & 10 & 10 & 54.47 & 39.95 & 66.75 & 48.24 & 42.16 & 22.31 & 56.51 & 31.41 & 58.73 & 42.29 \\\\\n",
    "\\midrule\n",
    "\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 10 examples or all training examples annotated by Gemma-3-27B, LLM-as-a-annotator (LLMA)}} \\\\\n",
    "\\midrule\n",
    "Paraphrase & 10 & 10 & 8.75 & 1.32 & 6.66 & 3.27 & 19.35 & 4.75 & 14.91 & 2.63 & 19.82 & 7.45 \\\\\n",
    "DLO & 10 & 10 & 15.84 & 4.37 & 13.59 & 6.85 & 25.61 & 4.47 & 18.07 & 3.53 & 22.14 & 9.26 \\\\\n",
    "LLMA w/ Paraphrase & full & full & 49.09 & 35.04 & 62.74 & 45.32 & 40.25 & 22.47 & 55.69 & 32.83 & 54.31 & 41.56 \\\\\n",
    "LLMA w/ DLO & full & full & 49.23 & 37.19 & 62.37 & 44.98 & 41.37 & 23.37 & 55.27 & 31.44 & 55.09 & 43.87 \\\\\n",
    "\\midrule\n",
    "\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 10 examples + EDA- or QAIE-augmented examples}} \\\\\n",
    "\\midrule\n",
    "EDA w/ Paraphrase & 30 & 30 & 13.15 & 1.74 & 7.67 & 3.95 & 22.48 & 6.61 & 22.05 & 3.47 & 24.37 & 10.23 \\\\\n",
    "EDA w/ Paraphrase & 60 & 60 & 23.20 & 10.42 & 16.35 & 8.76 & 27.59 & 9.47 & 25.85 & 6.15 & 28.91 & 15.74 \\\\\n",
    "EDA w/ Paraphrase & 110 & 110 & 26.07 & 10.52 & 16.82 & 9.05 & 30.17 & 10.35 & 24.95 & 5.84 & 29.85 & 16.47 \\\\\n",
    "EDA w/ DLO & 30 & 30 & 24.58 & 6.74 & 16.97 & 8.33 & 29.85 & 9.75 & 22.73 & 5.98 & 28.44 & 13.92 \\\\\n",
    "EDA w/ DLO & 60 & 60 & 26.65 & 8.68 & 18.06 & 9.14 & 31.36 & 11.73 & 23.95 & 7.51 & 30.79 & 17.36 \\\\\n",
    "EDA w/ DLO & 110 & 110 & 29.31 & 9.22 & 19.16 & 10.43 & 32.87 & 13.00 & 24.57 & 7.44 & 32.68 & 18.25 \\\\\n",
    "QAIE & 26.4 & 45.2 & 20.11 & 9.96 & 12.37 & 7.15 & 28.40 & 17.54 & 21.69 & 9.86 & 25.83 & 12.51 \\\\\n",
    "\\midrule\n",
    "\\midrule\n",
    "Gemma-3-27B (Prompting) & 50 & 50 & 62.12 & 41.74 & 68.53 & 49.87 & 45.87 & 25.86 & 62.97 & 43.83 & 67.45 & 53.21 \\\\\n",
    "\\midrule\n",
    "\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 50 examples or all training examples annotated by Gemma-3-27B, LLM-as-a-annotator (LLMA)}} \\\\\n",
    "\\midrule\n",
    "Paraphrase & 50 & 50 & 36.92 & 25.55 & 35.87 & 24.12 & 35.78 & 19.38 & 40.10 & 23.09 & 39.65 & 30.76 \\\\\n",
    "DLO & 50 & 50 & 39.54 & 26.63 & 43.95 & 28.44 & 38.46 & 19.08 & 44.72 & 27.20 & 43.88 & 33.47 \\\\\n",
    "LLMA w/ Paraphrase & full & full & 56.21 & 37.61 & 62.20 & 44.73 & 43.46 & 25.71 & 60.58 & 43.19 & 62.94 & 51.85 \\\\\n",
    "LLMA w/ DLO & full & full & 58.40 & 40.38 & 62.03 & 45.02 & 44.52 & 25.69 & 61.43 & 44.24 & 64.76 & 54.37 \\\\\n",
    "\\midrule\n",
    "\\multicolumn{13}{@{}l}{\\textit{SOTA approaches trained on 50 examples + EDA- or QAIE-augmented examples}} \\\\\n",
    "\\midrule\n",
    "EDA w/ Paraphrase & 150 & 150 & 42.70 & 27.55 & 41.59 & 27.85 & 39.73 & 19.90 & 43.44 & 27.96 & 45.28 & 34.60 \\\\\n",
    "EDA w/ Paraphrase & 300 & 300 & 42.29 & 28.96 & 42.57 & 28.62 & 40.25 & 21.08 & 44.67 & 28.04 & 46.12 & 35.97 \\\\\n",
    "EDA w/ Paraphrase & 550 & 550 & 41.83 & 28.70 & 43.01 & 28.77 & 40.09 & 21.37 & 42.70 & 27.97 & 45.76 & 35.42 \\\\\n",
    "EDA w/ DLO & 150 & 150 & 43.86 & 30.41 & 46.79 & 31.43 & 40.94 & 23.53 & 45.89 & 31.39 & 48.35 & 38.72 \\\\\n",
    "EDA w/ DLO & 300 & 300 & 44.76 & 30.90 & 46.99 & 31.99 & 41.53 & 23.53 & 45.20 & 32.55 & 49.03 & 39.87 \\\\\n",
    "EDA w/ DLO & 550 & 550 & 44.14 & 30.93 & 45.49 & 31.25 & 41.34 & 24.57 & 45.13 & 32.51 & 48.66 & 39.45 \\\\\n",
    "QAIE & 141.8 & 248.8 & 45.01 & 33.87 & 45.09 & 30.79 & 41.05 & 22.45 & 50.51 & 35.95 & 50.37 & 41.92 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}\n",
    "}\n",
    "\\caption{Performance comparison of different approaches on aspect-based sentiment analysis tasks across five datasets. Results are reported for two main metrics: TASD F1 (Target Aspect Sentiment Detection) and ASQP F1 (Aspect Sentiment Quad Prediction).}\n",
    "\\label{tab:results}\n",
    "\\end{table*}\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nils_hellwig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
